{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03a1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "69995       0.0       0.0       0.0       0.0       0.0  \n",
       "69996       0.0       0.0       0.0       0.0       0.0  \n",
       "69997       0.0       0.0       0.0       0.0       0.0  \n",
       "69998       0.0       0.0       0.0       0.0       0.0  \n",
       "69999       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version = 1, return_X_y=True)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "207bc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7692cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "# X -> 784 Pixels. 7000 instance\n",
    "# y -> more like a list for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e5497a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
       "       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,\n",
       "       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,\n",
       "       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,\n",
       "       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n",
       "       176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186.,\n",
       "       187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197.,\n",
       "       198., 199., 200., 201., 202., 203., 204., 205., 206., 207., 208.,\n",
       "       209., 210., 211., 212., 213., 214., 215., 216., 217., 218., 219.,\n",
       "       220., 221., 222., 223., 224., 225., 226., 227., 228., 229., 230.,\n",
       "       231., 232., 233., 234., 235., 236., 237., 238., 239., 240., 241.,\n",
       "       242., 243., 244., 245., 246., 247., 248., 249., 250., 251., 252.,\n",
       "       253., 254., 255.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9f014dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nomalizing the data points with the range of -1 to 1. Gradient based optimization is more stable\n",
    "#Under these conditions\n",
    "\n",
    "X = ((X/255.) - 0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bcbbc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 5, sharex=True, sharey = True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img = X[y == 7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap = 'Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd6935a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X[y == 4][0].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a65baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=10000, stratify=y, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d51c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, \n",
    "                                                      test_size=5000, \n",
    "                                                      stratify=y_temp, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7d8baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14121608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "        \n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h = sigmoid(z_h)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_out, y):  \n",
    "    \n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        \n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n",
    "\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # gradient for output weights\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "        \n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        \n",
    "\n",
    "        #################################        \n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out, \n",
    "                d_loss__d_w_h, d_loss__d_b_h)\n",
    "      \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82df1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(num_features=28*28,\n",
    "                     num_hidden=50,\n",
    "                     num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c51e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b452c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27072877992538014 0.0936\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "\n",
    "_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(mse, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a9bae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMseandAcc(nnet, X, y, num_labels = 10, minibatch_size = 100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    \n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "    \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis = 1)\n",
    "        \n",
    "        onehot_target = int_to_onehot(targets, num_labels=num_labels)\n",
    "        \n",
    "        loss = np.mean((onehot_target - probas)**2)\n",
    "        \n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        \n",
    "        mse += loss\n",
    "    \n",
    "    mse = mse/i\n",
    "    \n",
    "    acc = correct_pred/num_examples\n",
    "    \n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f1e4634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27625385706671457 0.0936\n"
     ]
    }
   ],
   "source": [
    "mse, acc = computeMseandAcc(model, X_valid, y_valid)\n",
    "print(mse, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
